<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Flexible Recurrent Feature Reasoning for Image Inpainting">
  <meta name="keywords" content="Image Inpainting, Recurrent Strategy, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Flexible Recurrent Feature Reasoning for Image Inpainting</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="shortcut icon" href="./favicon.ico">

  <meta property="og:site_name" content="Flexible Recurrent Feature Reasoning for Image Inpainting" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Flexible Recurrent Feature Reasoning for Image Inpainting" />
  <meta property="og:description" content="Wang, Zhang. Flexible Recurrent Feature Reasoning for Image Inpainting" />
  <meta property="og:url" content="https://wangning-001.github.io/FRFR/" />
  <meta property="og:image" content="https://worldsheet.github.io/static/images/preview.jpg" />
  <meta property="og:image:secure" content="https://worldsheet.github.io/static/images/preview.jpg" />
  <meta property="og:video" content="https://www.youtube.com/embed/j5aT3zRxFlk?rel=0&showinfo=0" />
  <meta property="og:video:secure" content="https://www.youtube.com/embed/j5aT3zRxFlk?rel=0&showinfo=0" />

  <meta property="article:publisher" content="https://pathak22.github.io" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Flexible Recurrent Feature Reasoning for Image Inpainting" />
  <meta name="twitter:description" content="Wang, Zhang. Flexible Recurrent Feature Reasoning for Image Inpainting" />
  <meta name="twitter:url" content="https://worldsheet.github.io/" />
  <meta name="twitter:image" content="https://worldsheet.github.io/static/images/preview.jpg" />
  <!-- <meta name="twitter:label1" content="Written by" />
  <meta name="twitter:data1" content="Deepak Pathak" /> -->
  <!-- <meta name="twitter:label2" content="Filed under" />
  <meta name="twitter:data2" content="" /> -->
  <meta name="twitter:site" content="@pathak2206" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="https://worldsheet.github.io/static/images/preview.jpg" />
  <meta name="twitter:player" content="https://www.youtube.com/embed/j5aT3zRxFlk" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Flexible Recurrent Feature Reasoning for Image Inpainting</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Ning Wang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a>‪Jingyuan Li‬</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="https://sites.google.com/site/lzhangpage/">Lefei Zhang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="http://cs.whu.edu.cn/aspx/enmain/teacherinfo.aspx?id=254">Bo Du</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng Tao</a><sup>2</sup>
              <br /><sup>1</sup>Wuhan University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>JD Explore Academy
              <span class="brmod"></span>CVPR 2020 Extension</span>
          </div>

<!--           <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Facebook AI Research,</span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./resources/FRFR.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/xxxx.xxxxxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
			  <!--
              <span class="link-block">
                <a href="#method_video"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
			  -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/facebookresearch/worldsheet" 
                   class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--
<section class="hero teaser">
  <div class="hero-body">
    <div class="columns is-centered">
      <div class="column is-8">
        <video id="teaser" autoplay muted loop height="100%">
          <source src="./resources/teaser.mp4"
                  type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Worldsheet synthesizes novel views of a scene from a <i>single image</i> using a mesh sheet for scene representation.
        </h2>
      </div>
    </div>
  </div>
</section>
-->

<!--
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop height="100%">
            <source src="./resources/compressed_no_mesh_Amtrak5_pad.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop height="100%">
            <source src="./resources/compressed_no_mesh_painting4_pad.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop height="100%">
            <source src="./resources/compressed_no_mesh_000000054605_pad.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="./resources/compressed_no_mesh_000000051738_pad.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
            <source src="./resources/compressed_no_mesh_alley_pad_wo_turnround.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop height="100%">
            <source src="./resources/compressed_no_mesh_painting1_pad.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->

<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing inpainting methods have achieved promising performance for recovering regular or small image defects. 
			However, filling in large continuous holes remains difficult due to the lack of constraints for the hole center. 
			To address this problem, we devise a Recurrent Feature Reasoning network (RFR-Net) which is mainly constructed by 
			a plug-and-play Recurrent Feature Reasoning module and a Knowledge Consistent Attention (KCA) module. Analogous to 
			how humans solve puzzles (i.e., first solve the easier parts and then use the results as additional information to 
			solve difficult parts), the RFR module recurrently infers the hole boundaries of the convolutional feature maps and 
			then uses them as clues for further inference. The module progressively strengthens the constraints for the hole 
			center and the results become explicit. To capture information from distant places in the feature map for RFR, we 
			further develop KCA and incorporate it in RFR. In addition, we propose Flexible RFR-Net (FRFR) that enhances the 
			effect without significantly increasing network parameters. In FRFR, to dynamically make use of valid information, 
			we employ a flexible normalization (FN) module, which combines three basic normalization methods to build regular FN 
			(FN-R) and mask-wise FN (FN-M). Extensive experiments comparing the performance with state-of-the-art image inpainting 
			methods demonstrate that both RFR and FRFR improve the qualitative scores, as well as significantly reduce the 
			distortion in quantitative results and obtain realistic images with fine details. What's more, the proposed RFR is 
			applied to the image super-resolution task, which demonstrates the superiority of the RFR module in various image 
			restoration tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->


  <!-- Paper video. -->
<!--
  <br/>
  <br/>
  <div id="method_video" class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-2">Video</h2>
      <div class="publication-video">
-->
<!--         <video controls>
          <source src="./resources/spotlight.mp4"
                  type="video/mp4">
        </video> -->
<!--
        <iframe src="https://www.youtube.com/embed/j5aT3zRxFlk?rel=0&showinfo=0&hd=1&vq=hd1080"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>
-->
  <!--/ Paper video. -->

</section>


<section class="section">
  <div class="container">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">Method</h2>
        <div class="content has-text-justified">
          <p>
            The masked image is first mapped into the convolutional feature space and processed by a shared Feature Reasoning module recurrently. 
			After the feature map is fully recovered, the generated feature maps are merged together (Omitted in this figure) and the merged feature is translated back to a RGB image.
          </p>
          <img src="./resources/overview.png" />
        </div>
        <br/>

        <!-- Interpolating. -->
        <h3 class="title is-4">Comparison to prior state-of-the-art</h3>
        <div class="content has-text-justified">
          <p>
            We evaluate and compare with previous approaches on three benchmark datasets: Matterport, Replica, and RealEstate10K. Our model outperforms previous work by a large margin under PSNR and other metrics (see paper for details).
          </p>
        </div>
        <div class="is-vcentered interpolation-panel">
          <div class="content has-text-centered">
            <img style="border: 1px solid #bbb; border-radius: 10px; width: 50%;" src="./resources/results.png">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<video style="border: 1px solid #bbb; border-radius: 10px; width: 44%;" autoplay muted loop>
              <source src="./resources/project_page_synsin_comparison.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="content has-text-centered">
            <p>
              Compared to <strong>SynSin (Wiles et al. 2020)</strong>, a prior state-of-the-art based on point cloud, our Worldsheet generalizes better to large viewpoint changes and has fewer artifacts.
            </p>
          </div>
        </div>
        <br/>
        <br/>

        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Failure cases</h3>
        <div class="content has-text-justified">
          <p>
            Sometimes artifacts occur when the depth is discontinuous around the object boundary. For instance, the boundary of the flower or the tree is blurry. We hope to address these issues in future work by segmenting the Worldsheet around depth boundaries.
          </p>
        </div>
        <div class="content has-text-centered">
          <video style="border: 1px solid #bbb; border-radius: 10px;"
                 controls
                 muted
                 width="35%">
            <source src="./resources/compressed_no_mesh_scenery11_pad.mp4"
                    type="video/mp4">
          </video>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <video style="border: 1px solid #bbb; border-radius: 10px;"
                 controls
                 muted
                 width="35%">
            <source src="./resources/compressed_no_mesh_paris_pad.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container content">
    <h2 class="titile">BibTeX</h2>
    <pre><code>@inproceedings{li2020recurrent,
  title={Recurrent feature reasoning for image inpainting},
  author={Li, Jingyuan and Wang, Ning and Zhang, Lefei and Du, Bo and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={7760--7768},
  year={2020}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a href="./resources/worldsheet.pdf" class="large-font bottom_buttons">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="large-font bottom_buttons" disabled>
        <i class="fab fa-github"></i>
      </a>
      <br />
      <p>Page template borrowed from <a href="https://nerfies.github.io/"><span class="dnerf">D-NeRF</span></a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
